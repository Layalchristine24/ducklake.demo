---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# DuckLake Demo

<!-- badges: start -->
<!-- badges: end -->

**The problem**: Traditional data lakes (just Parquet files) have no versioning, no transactions, no easy way to track changes.

**Existing solutions**: Iceberg and Delta Lake solve this, but they're complex â€” metadata stored in JSON/Avro files, need Spark clusters.

**DuckLake's approach**: Store metadata in a simple SQL database (SQLite), keep data in Parquet files. It's like having Git-like version control for your data, but queryable with SQL.

## Setup

```{r setup, message = FALSE}
library(duckplyr)

# Setup paths and clean slate
path <- "demo-ducklake"
unlink(path, recursive = TRUE)
dir.create(path, showWarnings = FALSE)

# Get a DuckDB connection
con <- duckplyr:::get_default_duckdb_connection()

# Load DuckLake extension
# DBI::dbExecute(con, "INSTALL ducklake")
DBI::dbExecute(con, "LOAD ducklake")
```

## 1. Create a DuckLake Database

Metadata is stored in SQLite, data in Parquet files.

```{r create-db}
DBI::dbExecute(
  con,
  sprintf("ATTACH 'ducklake:%s/metadata.ducklake' AS lake", path)
)

DBI::dbGetQuery(con, "SHOW DATABASES")
```

## 2. Create Table and Insert Data

```{r create-table}
DBI::dbExecute(
  con,
  "CREATE TABLE lake.customers (
    id INTEGER,
    name VARCHAR,
    email VARCHAR
  )"
)

DBI::dbExecute(
  con,
  "INSERT INTO lake.customers (id, name, email) VALUES
   (1, 'Alice', 'alice@example.com'),
   (2, 'Bob', 'bob@example.com'),
   (3, 'Charlie', 'charlie@example.com')"
)

# Query with duckplyr
tbl(con, I("lake.customers")) |> collect()
```

## 3. Time Travel

```{r time-travel}
# Get current snapshot version
snapshots <- DBI::dbGetQuery(con, "SELECT * FROM ducklake_snapshots('lake')")
snapshot_before <- max(snapshots$snapshot_id)

# Make changes
DBI::dbExecute(
  con,
  "UPDATE lake.customers SET email = 'alice.new@example.com' WHERE id = 1"
)
DBI::dbExecute(
  con,
  "INSERT INTO lake.customers VALUES (4, 'Diana', 'diana@example.com')"
)

# Current data (after changes)
tbl(con, I("lake.customers")) |> collect()

# Time travel to previous version
DBI::dbGetQuery(
  con,
  sprintf("SELECT * FROM lake.customers AT (VERSION => %s)", snapshot_before)
)
```

## 4. Schema Evolution

```{r schema-evolution}
DBI::dbExecute(
  con,
  "ALTER TABLE lake.customers ADD COLUMN status VARCHAR DEFAULT 'active'"
)

tbl(con, I("lake.customers")) |> collect()
```

## 5. MERGE INTO (Upsert)

```{r merge-into}
# Create source table with updates
DBI::dbExecute(
  con,
  "CREATE TEMP TABLE updates AS
   SELECT * FROM (VALUES
     (1, 'Alice', 'alice.updated@example.com', 'vip'),
     (5, 'Eve', 'eve@example.com', 'new')
   ) AS t(id, name, email, status)"
)

DBI::dbExecute(
  con,
  "MERGE INTO lake.customers AS target
   USING updates AS source
   ON target.id = source.id
   WHEN MATCHED THEN UPDATE SET
     email = source.email,
     status = source.status
   WHEN NOT MATCHED THEN INSERT (id, name, email, status)
     VALUES (source.id, source.name, source.email, source.status)"
)

tbl(con, I("lake.customers")) |> arrange(id) |> collect()
```

## 6. Metadata Inspection

```{r metadata}
# Version history
DBI::dbGetQuery(con, "SELECT * FROM ducklake_snapshots('lake')")

# Table info
# - file_count: number of Parquet data files
# - delete_file_count: number of positional delete files (created by UPDATE/DELETE/MERGE)
#   These track which rows have been logically removed without rewriting data files
DBI::dbGetQuery(con, "SELECT * FROM ducklake_table_info('lake')")
```

## Key Takeaways

1. **Metadata in SQL** - Simple, queryable metadata storage
2. **Parquet data files** - Efficient, portable data format
3. **Time Travel** - Auditing and recovery capabilities
4. **ACID transactions** - Data integrity guarantees
5. **Schema Evolution** - Add columns without rewriting data
6. **MERGE INTO** - Easy upsert operations

Perfect for data lakes that need versioning + transactions without the complexity of Iceberg/Delta Lake!

## Sources

- [DuckLake Manifesto](https://ducklake.select/manifesto/)
- [DuckLake GitHub Repository](https://github.com/duckdb/ducklake)
- [DuckLake Announcement Blog Post](https://duckdb.org/2025/05/27/ducklake)
- [DuckLake Documentation](https://duckdb.org/docs/stable/core_extensions/ducklake)
- [duckplyr Package](https://duckplyr.tidyverse.org/)

## Cleanup

```{r cleanup}
DBI::dbExecute(con, "DETACH lake")
DBI::dbDisconnect(con, shutdown = TRUE)
unlink(path, recursive = TRUE)
```
